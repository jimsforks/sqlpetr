---
title: "Building the `adventureworks` Docker Image"
author: "M. Edward (Ed) Borasky, John David Smith"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
      self_contained: true
bibliography: vignettes.bib
vignette: >
  %\VignetteIndexEntry{Building the `adventureworks` Docker Image}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction
In the _R, Databases and Docker_ eBook [@Smith2019], we use Docker containers to provide PostgreSQL database services. We use two containers:

1. `cattle`: A container with just PostgreSQL 11, and
2. `sql-pet`: A PostgreSQL 11 container with the `adventureworks` database [@Microsoft2018] pre-loaded.

Creating `cattle` is straightforward; we simply fetch the PostgreSQL 11 image [@Docker2018] and run it in a container. We do this in function `sqlpetr::sp_make_simple_pg`. However, the construction of the image used for `sql-pet` requires some more effort. The rest of this vignette describes the process.

## Converting a SQL Server backup to a PostgreSQL backup
The AdventureWorks2017 database is distributed in two forms - as a SQL Server backup file (`.bak` format) and as a zip archive containing CSV files with the data and a SQL Server script to create the database. Either way,
the end result will be a Microsoft SQL Server database containing the data.

So how do we get to a PostgreSQL backup? It's a four-step process:

1. Build / run a SQL Server Docker container with the AdventureWorks database restored to it.
2. Build / run a PostgreSQL Docker container.
3. Connect to the containers with R code and copy the relevant schemas from
SQL Server to PostgreSQL.
4. Create a backup file in the PostgreSQL container and retrieve it with `docker cp`.

## Building the SQL Server Docker container
The following code is based on [@Microsoft2019]

```{r}
# build the image
sqlpetr::sp_make_adventureworks_mssql_image(image_tag = "mssql")
sqlpetr::sp_docker_images_tibble()

# run the image in the container
sqlpetr::sp_docker_remove_container("mssql")
sqlpetr::sp_docker_run(
  image_tag = "mssql",
  options = paste(
    "-e 'ACCEPT_EULA=Y'",
    "-e 'MSSQL_SA_PASSWORD=sit-d0wn-COMIC'",
    "--name mssql",
    " --publish 1433:1433",
    "--detach",
    sep = " "
  )
)

# execute the restore script
sqlpetr::sp_docker_exec(
  container_name = "mssql",
  command = "/var/opt/mssql/backup/restore.bash"
)

# connect via ODBC
mssql_con <- DBI::dbConnect(
  odbc::odbc(),
  .connection_string = paste(
    "Driver={ODBC Driver 17 for SQL Server}",
    "Server=localhost",
    "Database=AdventureWorks2017",
    "UID=sa",
    "PWD=sit-d0wn-COMIC",
    sep = ";"
  ),
  timeout = 10
)

```

## Building the PostgreSQL image
```{r}
sqlpetr::sp_docker_remove_container("postgresql")
sqlpetr::sp_pg_docker_run(container_name = "postgresql", docker_network = "bridge")
postgresql_con <- sqlpetr::sp_get_postgres_connection(
  user = "postgres",
  password = "postgres",
  dbname = "postgres",
  host = "localhost",
  port = 5439,
  seconds_to_test = 30,
  connection_tab = FALSE
)
DBI::dbExecute(postgresql_con, "CREATE DATABASE adventureworks")
DBI::dbDisconnect(postgresql_con)
postgresql_con <- sqlpetr::sp_get_postgres_connection(
  user = "postgres",
  password = "postgres",
  dbname = "adventureworks",
  host = "localhost",
  port = 5439,
  seconds_to_test = 30,
  connection_tab = FALSE
)

```


## Copying the data
```{r}
library(dplyr)

# list the source tables
source_tables <- DBI::dbGetQuery(
  mssql_con,
  'SELECT * FROM "AdventureWorks2017"."INFORMATION_SCHEMA"."TABLES"'
) %>% dplyr::filter(
  TABLE_TYPE == "BASE TABLE",
  TABLE_SCHEMA != "dbo",
  TABLE_NAME != "ProductDocument",
  TABLE_NAME != "ProductPhoto",
  TABLE_NAME != "Address",
  TABLE_NAME != "Document",
  TABLE_NAME != "Employee"
)

# create the destination schemata
destination_schemata <- source_tables$TABLE_SCHEMA %>%
  tolower() %>% unique()
for (schema in destination_schemata) {
  DBI::dbExecute(
    postgresql_con,
    paste0("CREATE SCHEMA IF NOT EXISTS ", schema, ";")
  )
}

# copy the base tables - we are ignoring the views!
print(nrow(source_tables))
for (irow in 1:nrow(source_tables)) {
  
  # fetch source table
  catalog <- source_tables$TABLE_CATALOG[irow]
  schema <- source_tables$TABLE_SCHEMA[irow]
  tname <- source_tables$TABLE_NAME[irow]
  print(c(irow, catalog, schema, tname))
  source_query <- paste0(
    'SELECT * FROM "', catalog, '"."', schema, '"."', tname, '";'
  )
  work <- DBI::dbGetQuery(mssql_con, source_query)
  names(work) <- tolower(names(work))
  
  # write to correct destination schema
  DBI::dbExecute(
    postgresql_con,
    paste0("SET search_path TO ", tolower(schema), ";")
  )
  DBI::dbWriteTable(
    postgresql_con, tolower(tname), work, overwrite = TRUE)
}

```

## Creating the backup file

## References
